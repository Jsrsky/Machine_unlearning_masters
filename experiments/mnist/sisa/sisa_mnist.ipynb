{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:01.184184Z",
     "iopub.status.busy": "2024-11-29T14:18:01.181977Z",
     "iopub.status.idle": "2024-11-29T14:18:06.865219Z",
     "shell.execute_reply": "2024-11-29T14:18:06.864339Z",
     "shell.execute_reply.started": "2024-11-29T14:18:01.184142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import models, transforms, datasets\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device used: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.867241Z",
     "iopub.status.busy": "2024-11-29T14:18:06.866883Z",
     "iopub.status.idle": "2024-11-29T14:18:06.877818Z",
     "shell.execute_reply": "2024-11-29T14:18:06.877176Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.867214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "from utils.utils import set_seed\n",
    "set_seed()\n",
    "\n",
    "# Path to save models and metrics\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.879129Z",
     "iopub.status.busy": "2024-11-29T14:18:06.878824Z",
     "iopub.status.idle": "2024-11-29T14:18:06.885752Z",
     "shell.execute_reply": "2024-11-29T14:18:06.885111Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.879105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.887533Z",
     "iopub.status.busy": "2024-11-29T14:18:06.887286Z",
     "iopub.status.idle": "2024-11-29T14:18:06.899311Z",
     "shell.execute_reply": "2024-11-29T14:18:06.898686Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.887510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from models.simple_cnn import init_model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_metrics import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SISA structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.sisa.sisa_utils import create_sisa_structure\n",
    "\n",
    "from methods.sisa.sisa_utils import recreate_sisa_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, transform = init_model_cnn()\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.911823Z",
     "iopub.status.busy": "2024-11-29T14:18:06.911547Z",
     "iopub.status.idle": "2024-11-29T14:18:06.922581Z",
     "shell.execute_reply": "2024-11-29T14:18:06.921713Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.911797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SHARDS = 3\n",
    "SLICES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.923915Z",
     "iopub.status.busy": "2024-11-29T14:18:06.923683Z",
     "iopub.status.idle": "2024-11-29T14:18:06.936863Z",
     "shell.execute_reply": "2024-11-29T14:18:06.935934Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.923892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "create_sisa_structure(dataset, shards=SHARDS, slices_per_shard=SLICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sisa_structure_file = 'sisa_structure.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.938366Z",
     "iopub.status.busy": "2024-11-29T14:18:06.938074Z",
     "iopub.status.idle": "2024-11-29T14:18:06.952202Z",
     "shell.execute_reply": "2024-11-29T14:18:06.951221Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.938340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataloaders = recreate_sisa_dataloaders(datasets=(dataset, test_dataset), info_file_path = sisa_structure_file, batch_size=32, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_metrics import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:06.968752Z",
     "iopub.status.busy": "2024-11-29T14:18:06.968515Z",
     "iopub.status.idle": "2024-11-29T14:18:06.981436Z",
     "shell.execute_reply": "2024-11-29T14:18:06.980663Z",
     "shell.execute_reply.started": "2024-11-29T14:18:06.968729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sisa_test(model, model_path,  test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Inference on the test set\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f\"Evaluating model: {model_path}\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:18:21.997584Z",
     "iopub.status.busy": "2024-11-29T14:18:21.997311Z",
     "iopub.status.idle": "2024-11-29T14:19:47.772312Z",
     "shell.execute_reply": "2024-11-29T14:19:47.771411Z",
     "shell.execute_reply.started": "2024-11-29T14:18:21.997559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sisa_train(dataloaders, num_epochs, save_models_metrics_dir):\n",
    "\n",
    "    save_path = Path(save_models_metrics_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "    # Iterate over each shard\n",
    "    for shard_id, slices in dataloaders.items():\n",
    "        if shard_id == \"test\":  # Skip the test loader\n",
    "            continue\n",
    "            \n",
    "        print(f\"Training shard: {shard_id}\")\n",
    "        \n",
    "        # Initialize a new model for the shard\n",
    "        model, model_name, criterion, optimizer, _ = init_model_cnn()\n",
    "\n",
    "        # Iterate over slices in the shard\n",
    "        for slice_id, loaders in slices.items():\n",
    "\n",
    "            print(f\"  Training slice: {slice_id}\")\n",
    "            \n",
    "            # Get train and validation loaders for this slice\n",
    "            train_loader = loaders[\"train\"]\n",
    "            val_loader = loaders[\"val\"]\n",
    "\n",
    "            model_name = f\"{shard_id}_{slice_id}\" + model_name\n",
    "            \n",
    "            # Call the slice-level training function\n",
    "            train_model(\n",
    "                model=model, \n",
    "                model_name=model_name, \n",
    "                train_loader=train_loader, \n",
    "                val_loader=val_loader, \n",
    "                criterion=criterion, \n",
    "                optimizer=optimizer, \n",
    "                num_epochs=num_epochs\n",
    "                )\n",
    "\n",
    "        shard_model_path = f\"./{save_path}/{shard_id}_final_model.pth\"\n",
    "        torch.save(model.state_dict(), shard_model_path)\n",
    "        print(f\"Saved final shard model to {shard_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:19:47.773739Z",
     "iopub.status.busy": "2024-11-29T14:19:47.773457Z",
     "iopub.status.idle": "2024-11-29T14:19:53.993327Z",
     "shell.execute_reply": "2024-11-29T14:19:53.992518Z",
     "shell.execute_reply.started": "2024-11-29T14:19:47.773711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def sisa_test(dataloaders, saved_models_metrics_dir, clear_solo_models_preds=True):\n",
    "    test_loader = dataloaders[\"test\"]\n",
    "\n",
    "    # Initialize the evaluation results dictionary\n",
    "    evaluation_results = {}\n",
    "\n",
    "    for shard_id in [key for key in dataloaders.keys() if key != \"test\"]:\n",
    "\n",
    "        # Path to the final model for this shard\n",
    "        shard_model_path = f\"{saved_models_metrics_dir}/{shard_id}_final_model.pth\"\n",
    "\n",
    "        # Load model\n",
    "        model, model_name, *_ = init_model_cnn(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        model_name = f\"{shard_id}_\" + model_name\n",
    "\n",
    "        # Call the evaluation function\n",
    "        test_model(model, model_name, shard_model_path, test_loader)\n",
    "\n",
    "        intermediate_json_path = f\"{saved_models_metrics_dir}/{model_name}_predictions.json\"\n",
    "\n",
    "        # Load intermediate predictions JSON\n",
    "        with open(intermediate_json_path, \"r\") as f:\n",
    "            shard_data = json.load(f)\n",
    "\n",
    "        # Add shard-specific predictions and true labels to evaluation_results\n",
    "        evaluation_results[shard_id] = {\n",
    "            \"predictions\": [int(p) for p in shard_data[\"predictions\"]],\n",
    "            \"true_labels\": [int(t) for t in shard_data[\"true_labels\"]]\n",
    "        }\n",
    "\n",
    "        # Delete intermediate JSON if clear_solo_models_preds is True\n",
    "        if clear_solo_models_preds:\n",
    "            os.remove(intermediate_json_path)\n",
    "\n",
    "    # Save predictions and true labels to a JSON file\n",
    "    output_path = \"sisa_final_evaluation.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(evaluation_results, f)\n",
    "\n",
    "    print(f\"Evaluation results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:19:53.995143Z",
     "iopub.status.busy": "2024-11-29T14:19:53.994502Z",
     "iopub.status.idle": "2024-11-29T14:19:54.000094Z",
     "shell.execute_reply": "2024-11-29T14:19:53.999308Z",
     "shell.execute_reply.started": "2024-11-29T14:19:53.995101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_shard_metrics(true_labels, shard_predictions):\n",
    "    \"\"\"\n",
    "    Calculate metrics for a single shard's predictions.\n",
    "    \n",
    "    Args:\n",
    "        true_labels (list): True labels for the dataset.\n",
    "        shard_predictions (list): Predictions from a shard model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(true_labels, shard_predictions)\n",
    "    precision = precision_score(true_labels, shard_predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(true_labels, shard_predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, shard_predictions, average=\"weighted\", zero_division=0)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:44:39.762745Z",
     "iopub.status.busy": "2024-11-29T14:44:39.762441Z",
     "iopub.status.idle": "2024-11-29T14:44:39.768888Z",
     "shell.execute_reply": "2024-11-29T14:44:39.767958Z",
     "shell.execute_reply.started": "2024-11-29T14:44:39.762720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_predictions(true_labels, predictions, weights):\n",
    "    \"\"\"\n",
    "    Perform weighted voting aggregation of shard predictions.\n",
    "    \n",
    "    Args:\n",
    "        true_labels (list): True labels for the dataset.\n",
    "        predictions (dict): Dictionary of shard predictions.\n",
    "        weights (list): List of weights (accuracies) for each shard.\n",
    "    \n",
    "    Returns:\n",
    "        list: Aggregated predictions.\n",
    "    \"\"\"\n",
    "    num_samples = len(true_labels)\n",
    "    num_shards = len(predictions)\n",
    "    \n",
    "    # Create an array to store shard predictions for each sample\n",
    "    shard_preds = np.array([predictions[f\"shard_{i}\"] for i in range(num_shards)])  # Shape: (num_shards, num_samples)\n",
    "    weights = np.array(weights).reshape(-1, 1)  # Shape: (num_shards, 1)\n",
    "    \n",
    "    # Weighted voting: Take the weighted mode of predictions\n",
    "    weighted_votes = np.zeros((10, num_samples))  # Assuming 10 classes (MNIST)\n",
    "    for shard_idx in range(num_shards):\n",
    "        for i, pred in enumerate(shard_preds[shard_idx]):\n",
    "            pred =pred.item()\n",
    "            i = int(i)\n",
    "            weighted_votes[pred, i] += weights[shard_idx]\n",
    "    \n",
    "    # Final prediction: Class with the highest weighted vote\n",
    "    aggregated_predictions = np.argmax(weighted_votes, axis=0)\n",
    "    return aggregated_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:23:03.203757Z",
     "iopub.status.busy": "2024-11-29T14:23:03.203412Z",
     "iopub.status.idle": "2024-11-29T14:23:03.212599Z",
     "shell.execute_reply": "2024-11-29T14:23:03.211587Z",
     "shell.execute_reply.started": "2024-11-29T14:23:03.203725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_aggregated_model(results):\n",
    "    \"\"\"\n",
    "    Evaluate aggregated predictions and print metrics, confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Results dictionary containing shard-specific predictions and true_labels.\n",
    "    \"\"\"\n",
    "    # Initialize variables for aggregated predictions and shard metrics\n",
    "    shard_metrics = {}\n",
    "    shard_accuracies = []\n",
    "    aggregated_predictions = []\n",
    "    true_labels = None\n",
    "\n",
    "    # Evaluate each shard\n",
    "    for shard_id, shard_data in results.items():\n",
    "        shard_preds = shard_data[\"predictions\"]\n",
    "        shard_true_labels = shard_data[\"true_labels\"]\n",
    "\n",
    "        # Ensure true_labels are consistent across shards\n",
    "        if true_labels is None:\n",
    "            true_labels = shard_true_labels\n",
    "        elif true_labels != shard_true_labels:\n",
    "            raise ValueError(f\"True labels in shard {shard_id} do not match other shards!\")\n",
    "\n",
    "        # Calculate metrics for the shard\n",
    "        metrics = calculate_shard_metrics(true_labels, shard_preds)\n",
    "        shard_metrics[shard_id] = metrics\n",
    "        shard_accuracies.append(metrics[\"accuracy\"])\n",
    "        print(f\"Shard {shard_id} Metrics:\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "\n",
    "    aggregated_predictions = aggregate_predictions(true_labels, aggregated_predictions)\n",
    "\n",
    "    # Calculate metrics for the aggregated predictions\n",
    "    aggregated_metrics = calculate_shard_metrics(true_labels, aggregated_predictions)\n",
    "    print(\"\\nAggregated Model Metrics:\")\n",
    "    print(f\"  Accuracy: {aggregated_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {aggregated_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {aggregated_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {aggregated_metrics['f1_score']:.4f}\")\n",
    "\n",
    "    # Generate and display confusion matrix\n",
    "    cm = confusion_matrix(true_labels, aggregated_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sb.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix of Aggregated Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:28:20.794502Z",
     "iopub.status.busy": "2024-11-29T14:28:20.794144Z",
     "iopub.status.idle": "2024-11-29T14:28:20.802235Z",
     "shell.execute_reply": "2024-11-29T14:28:20.801564Z",
     "shell.execute_reply.started": "2024-11-29T14:28:20.794471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"sisa_final_evaluation.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:44:45.303377Z",
     "iopub.status.busy": "2024-11-29T14:44:45.303036Z",
     "iopub.status.idle": "2024-11-29T14:44:45.942509Z",
     "shell.execute_reply": "2024-11-29T14:44:45.941574Z",
     "shell.execute_reply.started": "2024-11-29T14:44:45.303347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_aggregated_model(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def update_sisa_structure(unlearn_samples_path, sisa_structure_path, updated_structure_path, deleted_samples_path):\n",
    "    \"\"\"\n",
    "    Handles unlearning by identifying affected shards and slices, updating SISA structure,\n",
    "    and triggering retraining and testing.\n",
    "\n",
    "    Args:\n",
    "        unlearn_samples_path (str): Path to JSON file with samples to unlearn.\n",
    "        sisa_structure_path (str): Path to SISA structure JSON file.\n",
    "        updated_structure_path (str): Path to save updated SISA structure JSON file.\n",
    "        deleted_samples_path (str): Path to save deleted samples JSON file.\n",
    "    \"\"\"\n",
    "    # Load samples to unlearn\n",
    "    with open(unlearn_samples_path, \"r\") as f:\n",
    "        unlearn_samples = json.load(f)\n",
    "\n",
    "    # Load the SISA structure\n",
    "    with open(sisa_structure_path, \"r\") as f:\n",
    "        sisa_structure = json.load(f)\n",
    "\n",
    "    # Track affected shards, slices, and samples to delete\n",
    "    affected_shards = {}\n",
    "    deleted_samples = []\n",
    "\n",
    "    # Identify affected shards and slices\n",
    "    for sample in unlearn_samples:\n",
    "        index, label = sample[\"index\"], sample[\"class\"]\n",
    "        for shard, slices in sisa_structure.items():\n",
    "            for slice_name, slice_data in slices.items():\n",
    "                if index in slice_data[\"indices\"] and label in slice_data[\"classes\"]:\n",
    "                    # Track the lowest affected slice\n",
    "                    if shard not in affected_shards:\n",
    "                        affected_shards[shard] = []\n",
    "                    affected_shards[shard].append(slice_name)\n",
    "\n",
    "                    # Remove the sample from the slice\n",
    "                    idx_position = slice_data[\"indices\"].index(index)\n",
    "                    slice_data[\"indices\"].pop(idx_position)\n",
    "                    slice_data[\"classes\"].pop(idx_position)\n",
    "\n",
    "                    # Track deleted samples\n",
    "                    deleted_samples.append(sample)\n",
    "                    break  # Move to the next sample after finding a match\n",
    "\n",
    "    # Deduplicate and sort slice flags\n",
    "    affected_shards = {\n",
    "    shard: sorted(set(affected_shards[shard]))\n",
    "    for shard in sorted(affected_shards)\n",
    "}\n",
    "\n",
    "    # Save the updated SISA structure\n",
    "    with open(updated_structure_path, \"w\") as f:\n",
    "        json.dump(sisa_structure, f)\n",
    "\n",
    "    # Save the deleted samples\n",
    "    with open(deleted_samples_path, \"w\") as f:\n",
    "        json.dump(deleted_samples, f)\n",
    "\n",
    "    # Print retraining plan\n",
    "    print(\"Retraining Plan:\")\n",
    "    for shard, slices in affected_shards.items():\n",
    "        print(f\"  Shard: {shard}, Start from Slice: {slices[0]} onward\")\n",
    "\n",
    "    return affected_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mnist_samples_to_delete = 'mnist_samples_to_unlearn.json'\n",
    "mnist_sisa_structure = 'sisa_structure.json'\n",
    "updated_sisa_structure = 'updated_sisa_strucute.json'\n",
    "deleted_samples = 'deleted_samples.json'\n",
    "\n",
    "affected_shards = update_sisa_structure(mnist_samples_to_delete, mnist_sisa_structure, updated_sisa_structure,deleted_samples)\n",
    "print(affected_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_sisa_framework(updated_structure_path, affected_shards, dataset, save_path=\"./sisa_models\"):\n",
    "    \"\"\"\n",
    "    Retrain the SISA framework starting from the flagged slices for affected shards.\n",
    "\n",
    "    Args:\n",
    "        updated_structure_path (str): Path to the updated SISA structure JSON file.\n",
    "        affected_shards (dict): Dictionary of affected shards and their flagged slices.\n",
    "        dataset (Dataset): Original dataset to recreate dataloaders.\n",
    "        save_path (str): Path to save the updated models.\n",
    "    \"\"\"\n",
    "    # Load the updated SISA structure\n",
    "    with open(updated_structure_path, \"r\") as f:\n",
    "        updated_structure = json.load(f)\n",
    "\n",
    "    # Recreate dataloaders\n",
    "    dataloaders = recreate_sisa_dataloaders(updated_structure, dataset)\n",
    "\n",
    "    # Loop through affected shards\n",
    "    for shard_id, flagged_slices in affected_shards.items():\n",
    "        print(f\"Retraining shard: {shard_id}, starting from slice: slice_{flagged_slices[0]}\")\n",
    "\n",
    "        # Initialize a new model for this shard\n",
    "        model, criterion, optimizer = init_model()\n",
    "\n",
    "        # Retrain starting from the lowest flagged slice\n",
    "        for slice_id, loaders in dataloaders[shard_id].items():\n",
    "            # Check if the slice ID is >= the lowest flagged slice\n",
    "            current_slice_idx = int(slice_id.split(\"_\")[1])\n",
    "            if current_slice_idx >= flagged_slices[0]:\n",
    "                print(f\"  Retraining slice: {slice_id}\")\n",
    "\n",
    "                # Get train and validation loaders\n",
    "                train_loader = loaders[\"train\"]\n",
    "                val_loader = loaders[\"val\"]\n",
    "\n",
    "                # Train on this slice\n",
    "                sisa_train(\n",
    "                    model=model,\n",
    "                    shard_id=shard_id,\n",
    "                    slice_id=slice_id,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    criterion=criterion,\n",
    "                    optimizer=optimizer,\n",
    "                    num_epochs=EPOCHS,\n",
    "                    save_path=save_path\n",
    "                )\n",
    "\n",
    "        # Save the final model for the shard\n",
    "        shard_model_path = f\"{save_path}/{shard_id}_final_model.pth\"\n",
    "        torch.save(model.state_dict(), shard_model_path)\n",
    "        print(f\"Saved updated model for {shard_id} to {shard_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
